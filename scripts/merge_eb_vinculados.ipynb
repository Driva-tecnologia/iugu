{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49024051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, rand\n",
    "from pyspark.sql.functions import round as ps_round\n",
    "from pyspark.sql.types import StructField, StringType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bddd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 09:50:12 WARN Utils: Your hostname, greca resolves to a loopback address: 127.0.1.1; using 192.168.18.13 instead (on interface enp7s0)\n",
      "25/05/07 09:50:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/greca/anaconda3/envs/driva_ecomm/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/greca/.ivy2/cache\n",
      "The jars for the packages stored in: /home/greca/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-49a38ede-1c00-448d-990c-0013866795e2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      ":: resolution report :: resolve 330ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-49a38ede-1c00-448d-990c-0013866795e2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/8ms)\n",
      "25/05/07 09:50:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/07 09:50:14 WARN DependencyUtils: Local jar /home/shared/drivers/postgresql-42.7.2.jar does not exist, skipping.\n",
      "25/05/07 09:50:14 INFO SparkContext: Running Spark version 3.4.0\n",
      "25/05/07 09:50:14 INFO ResourceUtils: ==============================================================\n",
      "25/05/07 09:50:14 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/05/07 09:50:14 INFO ResourceUtils: ==============================================================\n",
      "25/05/07 09:50:14 INFO SparkContext: Submitted application: App\n",
      "25/05/07 09:50:14 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 61440, script: , vendor: , pyspark.memory -> name: pyspark.memory, amount: 61440, script: , vendor: , offHeap -> name: offHeap, amount: 61440, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/05/07 09:50:14 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/05/07 09:50:14 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/05/07 09:50:14 INFO SecurityManager: Changing view acls to: greca\n",
      "25/05/07 09:50:14 INFO SecurityManager: Changing modify acls to: greca\n",
      "25/05/07 09:50:14 INFO SecurityManager: Changing view acls groups to: \n",
      "25/05/07 09:50:14 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/05/07 09:50:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: greca; groups with view permissions: EMPTY; users with modify permissions: greca; groups with modify permissions: EMPTY\n",
      "25/05/07 09:50:15 INFO Utils: Successfully started service 'sparkDriver' on port 35507.\n",
      "25/05/07 09:50:15 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/05/07 09:50:15 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/05/07 09:50:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/05/07 09:50:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/05/07 09:50:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/05/07 09:50:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3d6333b5-d26d-4871-b058-4112dc3aa2ed\n",
      "25/05/07 09:50:15 INFO MemoryStore: MemoryStore started with capacity 91.8 GiB\n",
      "25/05/07 09:50:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/05/07 09:50:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/05/07 09:50:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/05/07 09:50:15 ERROR SparkContext: Failed to add /home/shared/drivers/postgresql-42.7.2.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/shared/drivers/postgresql-42.7.2.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/05/07 09:50:15 INFO SparkContext: Added file file:///home/greca/.ivy2/jars/io.delta_delta-spark_2.12-3.1.0.jar at file:///home/greca/.ivy2/jars/io.delta_delta-spark_2.12-3.1.0.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:15 INFO Utils: Copying /home/greca/.ivy2/jars/io.delta_delta-spark_2.12-3.1.0.jar to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/io.delta_delta-spark_2.12-3.1.0.jar\n",
      "25/05/07 09:50:15 INFO SparkContext: Added file file:///home/greca/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.2.2.jar at file:///home/greca/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.2.2.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:15 INFO Utils: Copying /home/greca/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.2.2.jar to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/org.apache.hadoop_hadoop-aws-3.2.2.jar\n",
      "25/05/07 09:50:15 INFO SparkContext: Added file file:///home/greca/.ivy2/jars/io.delta_delta-storage-3.1.0.jar at file:///home/greca/.ivy2/jars/io.delta_delta-storage-3.1.0.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:15 INFO Utils: Copying /home/greca/.ivy2/jars/io.delta_delta-storage-3.1.0.jar to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/io.delta_delta-storage-3.1.0.jar\n",
      "25/05/07 09:50:15 INFO SparkContext: Added file file:///home/greca/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar at file:///home/greca/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:15 INFO Utils: Copying /home/greca/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "25/05/07 09:50:15 INFO SparkContext: Added file file:///home/greca/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar at file:///home/greca/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:15 INFO Utils: Copying /home/greca/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar\n",
      "25/05/07 09:50:16 INFO Executor: Starting executor ID driver on host 192.168.18.13\n",
      "25/05/07 09:50:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/05/07 09:50:16 INFO Executor: Fetching file:///home/greca/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:16 INFO Utils: /home/greca/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar has been previously copied to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/com.amazonaws_aws-java-sdk-bundle-1.11.563.jar\n",
      "25/05/07 09:50:16 INFO Executor: Fetching file:///home/greca/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:16 INFO Utils: /home/greca/.ivy2/jars/org.antlr_antlr4-runtime-4.9.3.jar has been previously copied to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/org.antlr_antlr4-runtime-4.9.3.jar\n",
      "25/05/07 09:50:16 INFO Executor: Fetching file:///home/greca/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.2.2.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:16 INFO Utils: /home/greca/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.2.2.jar has been previously copied to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/org.apache.hadoop_hadoop-aws-3.2.2.jar\n",
      "25/05/07 09:50:16 INFO Executor: Fetching file:///home/greca/.ivy2/jars/io.delta_delta-spark_2.12-3.1.0.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:16 INFO Utils: /home/greca/.ivy2/jars/io.delta_delta-spark_2.12-3.1.0.jar has been previously copied to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/io.delta_delta-spark_2.12-3.1.0.jar\n",
      "25/05/07 09:50:16 INFO Executor: Fetching file:///home/greca/.ivy2/jars/io.delta_delta-storage-3.1.0.jar with timestamp 1746622214492\n",
      "25/05/07 09:50:16 INFO Utils: /home/greca/.ivy2/jars/io.delta_delta-storage-3.1.0.jar has been previously copied to /tmp/spark-e068f60b-9391-4a48-be6a-f5b9ce510060/userFiles-45970b3a-8815-41c5-a8ba-dd5ebafb7f58/io.delta_delta-storage-3.1.0.jar\n",
      "25/05/07 09:50:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35973.\n",
      "25/05/07 09:50:16 INFO NettyBlockTransferService: Server created on 192.168.18.13:35973\n",
      "25/05/07 09:50:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/05/07 09:50:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.18.13, 35973, None)\n",
      "25/05/07 09:50:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.18.13:35973 with 91.8 GiB RAM, BlockManagerId(driver, 192.168.18.13, 35973, None)\n",
      "25/05/07 09:50:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.18.13, 35973, None)\n",
      "25/05/07 09:50:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.18.13, 35973, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session configurada com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 09:50:17 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.NoClassDefFoundError: org/apache/spark/sql/catalyst/analysis/UnresolvedLeafNode\n",
      "\tat java.lang.ClassLoader.defineClass1(Native Method)\n",
      "\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n",
      "\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n",
      "\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:473)\n",
      "\tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.lang.ClassLoader.defineClass1(Native Method)\n",
      "\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n",
      "\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n",
      "\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:473)\n",
      "\tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat java.lang.ClassLoader.defineClass1(Native Method)\n",
      "\tat java.lang.ClassLoader.defineClass(ClassLoader.java:756)\n",
      "\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n",
      "\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:473)\n",
      "\tat java.net.URLClassLoader.access$100(URLClassLoader.java:74)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:369)\n",
      "\tat java.net.URLClassLoader$1.run(URLClassLoader.java:363)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:362)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\tat org.apache.spark.sql.delta.DeltaTableValueFunctions$.getTableValueFunctionInjection(DeltaTableValueFunctions.scala:58)\n",
      "\tat io.delta.sql.DeltaSparkSessionExtension.$anonfun$apply$14(DeltaSparkSessionExtension.scala:167)\n",
      "\tat io.delta.sql.DeltaSparkSessionExtension.$anonfun$apply$14$adapted(DeltaSparkSessionExtension.scala:165)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat io.delta.sql.DeltaSparkSessionExtension.apply(DeltaSparkSessionExtension.scala:165)\n",
      "\tat io.delta.sql.DeltaSparkSessionExtension.apply(DeltaSparkSessionExtension.scala:81)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1(SparkSession.scala:1297)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$1$adapted(SparkSession.scala:1292)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1292)\n",
      "\tat org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:107)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.catalyst.analysis.UnresolvedLeafNode\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n",
      "\t... 58 more\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"App\").setMaster(\"local[*]\")\n",
    "\n",
    "# Habilitar otimizações e configurações adicionais\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "conf.set(\"spark.sql.repl.eagerEval.truncate\", 100)\n",
    "conf.set(\"spark.sql.execution.arrow.pyspark.ignore_timezone\", \"true\")\n",
    "\n",
    "# AWS S3 CONNECTION\n",
    "AWS_ENDPOINT_URL = \"https://s3.bhs.io.cloud.ovh.net\"\n",
    "AWS_ACCESS_KEY = \"\"\n",
    "AWS_SECRET_KEY = \"\"\n",
    "AWS_REGION = \"bhs\"\n",
    "\n",
    "conf.set(\"spark.jars\", \"/home/shared/drivers/postgresql-42.7.2.jar\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_ENDPOINT_URL)\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "conf.set(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "\n",
    "conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "\n",
    "# Configurações de tempo e legacy\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\",\"LEGACY\")\n",
    "\n",
    "# Configurações de memória\n",
    "conf.set(\"spark.driver.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.memory\", \"60g\")\n",
    "conf.set(\"spark.executor.pyspark.memory\", \"60g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"60g\")\n",
    "\n",
    "# Inicializa o SparkSession com a configuração\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "print(\"Spark session configurada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_HOST = \"driva-db.driva.io\"\n",
    "DB_PORT = 5432\n",
    "DB_NAME = \"postgres\"\n",
    "DB_SITES_SCHEMA = \"sites.vinculados\"\n",
    "DB_EB_DRIVA = \"empresas_do_brasil.simplificado\"\n",
    "DB_USER = \"rafael_greca\"\n",
    "DB_PASSWORD = \"59tzyowc#*IGk\"\n",
    "\n",
    "sites_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\") \\\n",
    "    .option(\"dbtable\", DB_SITES_SCHEMA) \\\n",
    "    .option(\"user\", DB_USER) \\\n",
    "    .option(\"password\", DB_PASSWORD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "eb_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\") \\\n",
    "    .option(\"dbtable\", DB_EB_DRIVA) \\\n",
    "    .option(\"user\", DB_USER) \\\n",
    "    .option(\"password\", DB_PASSWORD) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a67c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = sites_df.filter(col(\"score_vinculo\") >= 0.7)\n",
    "sites_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b74fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_df = eb_df.filter(\n",
    "    (col(\"cnae_principal_divisao\") == 85) | # Educação\n",
    "    (col(\"cnae_principal_divisao\") == 86) | # Saúde\n",
    "    (col(\"cnae_principal_classe\") == 62023) | # Software / Saas\n",
    "    (col(\"cnae_principal_classe\") == 62031) # Software / Saas\n",
    ")\n",
    "eb_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = sites_df.join(eb_df, on=\"raiz_cnpj\", how=\"inner\")\n",
    "merged_df = merged_df.select(\n",
    "    \"dominio\",\n",
    "    \"raiz_cnpj\",\n",
    "    \"cnpj\",\n",
    "    \"cnae_principal_divisao\",\n",
    "    \"cnae_principal_secao\",\n",
    "    \"cnae_principal_grupo\",\n",
    "    \"cnae_principal_classe\",\n",
    "    \"cnae_principal_subclasse\",\n",
    "    \"segmento\",\n",
    ")\n",
    "merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afe532",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.createDataFrame(\n",
    "    merged_df.rdd.takeSample(\n",
    "        withReplacement=False,\n",
    "        num=5000,\n",
    "        seed=42,\n",
    "    ),\n",
    "    schema=merged_df.schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bae725",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnae_divisions = [85, 86, 62]\n",
    "num_samples = 10000\n",
    "seed = 42\n",
    "\n",
    "new_df = spark.createDataFrame(\n",
    "    spark.sparkContext.emptyRDD(),\n",
    "    schema=merged_df.schema,\n",
    ")\n",
    "\n",
    "# field = [\n",
    "#     StructField(\"raiz_cnpj\", StringType(), True),\n",
    "#     StructField(\"raiz_cnpj\", StringType(), True),\n",
    "#     StructField(\"Segmento iugu\", StringType(), True),\n",
    "# ]\n",
    "# schema = StructType(field)\n",
    "\n",
    "# new_segment_column = spark.createDataFrame(\n",
    "#     spark.sparkContext.emptyRDD(),\n",
    "#     schema=schema,\n",
    "# )\n",
    "\n",
    "for cnae in cnae_divisions:\n",
    "    if cnae in [85, 86]:\n",
    "        temp = merged_df.filter(col(\"cnae_principal_divisao\") == cnae)\n",
    "        temp = spark.createDataFrame(\n",
    "            temp.rdd.takeSample(\n",
    "                withReplacement=False,\n",
    "                num=num_samples,\n",
    "                seed=seed,\n",
    "            ),\n",
    "            schema=merged_df.schema,\n",
    "        )\n",
    "\n",
    "        # if cnae == 85:\n",
    "        #     temp_segment = temp.withColumn(\"Segmento iugu\", lit(\"Educação\")).select(\"raiz_cnpj\", \"cnpj\", \"Segmento iugu\")\n",
    "        # elif cnae == 86:\n",
    "        #     temp_segment = temp.withColumn(\"Segmento iugu\", lit(\"Saúde\")).select(\"raiz_cnpj\", \"cnpj\", \"Segmento iugu\")\n",
    "\n",
    "    else:\n",
    "        temp = merged_df.filter(\n",
    "            (col(\"cnae_principal_classe\") == 62023) |\n",
    "            (col(\"cnae_principal_classe\") == 62031)\n",
    "        )\n",
    "        \n",
    "        temp = spark.createDataFrame(\n",
    "            temp.rdd.takeSample(\n",
    "                withReplacement=False,\n",
    "                num=num_samples,\n",
    "                seed=seed,\n",
    "            ),\n",
    "            schema=merged_df.schema,\n",
    "        )\n",
    "        # temp_segment = temp.withColumn(\"Segmento iugu\", lit(\"Saas\")).select(\"raiz_cnpj\", \"cnpj\", \"Segmento iugu\")\n",
    "        \n",
    "    new_df = new_df.union(temp)\n",
    "    # new_segment_column = new_segment_column.union(temp_segment)\n",
    "\n",
    "# new_df = new_df.join(new_segment_column, on=\"raiz_cnpj\", how=\"inner\")\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4522e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.write.save(\"s3a://drivalake/raw/sites/iugu/iugu_chatgpt.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d601946",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfc3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619fbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"../data/new_iugu_saas2_with_html.parquet\", engine=\"pyarrow\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289318a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_parquet(\"../data/iugu_with_saas2.parquet\", engine=\"pyarrow\")\n",
    "label = label.rename(columns={\"dominio\": \"host\"})\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece06c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(label, how=\"inner\", on=\"host\")\n",
    "df = df[\n",
    "    [\n",
    "        \"url\",\n",
    "        \"host\",\n",
    "        \"html\",\n",
    "        \"raiz_cnpj\",\n",
    "        \"cnpj\",\n",
    "        \"cnae_principal_divisao\",\n",
    "        \"cnae_principal_secao\",\n",
    "        \"cnae_principal_grupo\",\n",
    "        \"cnae_principal_classe\",\n",
    "        \"cnae_principal_subclasse\",\n",
    "        \"segmento\",\n",
    "    ]\n",
    "]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88675db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    85: \"Educação\",\n",
    "    86: \"Saúde\",\n",
    "    62: \"Saas\"\n",
    "}\n",
    "\n",
    "df[\"Segmento iugu\"] = df[\"cnae_principal_divisao\"].apply(lambda x: mapping[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51def5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Segmento iugu\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"../data/new_iugu_saas2_with_html.parquet\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
