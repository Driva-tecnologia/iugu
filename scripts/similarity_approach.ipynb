{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/new_iugu_saas2_with_html.parquet\", engine=\"pyarrow\")\n",
    "df[\"html\"] = df[\"html\"].astype(str)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d16457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844538c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Segmento iugu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Nicho Tech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f682e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"host\", \"url\", \"html\", \"Segmento iugu\"]]\n",
    "df = df.rename(columns={\"Segmento iugu\": \"segment\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c14a5",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'host',\n",
    "            'html',\n",
    "            'url',\n",
    "        ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '')]\n",
    "    \n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates(subset=[\"host\"])\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "    \n",
    "        nulls = dataframe['host'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'host' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['host'])\n",
    "\n",
    "        nulls = dataframe['url'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'url' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['url'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "      from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens_lemmatized = []\n",
    "      for token in tokens:\n",
    "        if token in lemmatizer_pt_dict.keys():\n",
    "          tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "        else:\n",
    "          tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_vectorizer(html_text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        # pegar somente o body do HTML\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text = soup.body.get_text() if soup.body else ''\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def process_html_for_how_many_prices(text):\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))\n",
    "\n",
    "def process_html_for_how_many_values(text):\n",
    "    try:              \n",
    "        regex_valores = re.compile(r'\\d+(?:\\.\\d{3})*(?:,\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
    "        valores = regex_valores.findall(text)\n",
    "        return len(valores)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for values.\\nError:\\n' + str(e))\n",
    "\n",
    "def get_html_body(html_str):\n",
    "    try:\n",
    "        # Tentar usar diferentes parsers\n",
    "        for parser in ['html.parser', 'html5lib', 'lxml']:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, parser)\n",
    "                text = soup.body.get_text() if soup.body else ''\n",
    "                return text\n",
    "            except Exception as parser_e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataframe):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "\n",
    "        lem_dict = build_lemmatizer_pt_dict()\n",
    "        dataframe.loc[:, 'tokens'] = dataframe.loc[:, 'html'].apply(lambda x: process_html_for_vectorizer(x, lem_dict))\n",
    "\n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(score, confusion_matrix, classification_report, classes):\n",
    "    # Gera o heatmap da confusion matrix\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt=\"d\", \n",
    "                linewidths=.5, \n",
    "                square = True, \n",
    "                cmap = 'Blues', \n",
    "                annot_kws={\"size\": 16}, \n",
    "                xticklabels=classes, \n",
    "                yticklabels=classes)\n",
    "\n",
    "    plt.xticks(rotation='horizontal', fontsize=16)\n",
    "    plt.yticks(rotation='horizontal', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', size=20)\n",
    "    plt.ylabel('Actual Label', size=20)\n",
    "\n",
    "    title = 'Accuracy Score: {0:.4f}'.format(score)\n",
    "    plt.title(title, size = 20)\n",
    "\n",
    "    # Mostra o classification report e o heatmap\n",
    "    pprint(classification_report)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(vectorizer, vectors):\n",
    "    try:\n",
    "        file_name = \"iugu_tfidf_similarity_1000_base_iugu\"\n",
    "\n",
    "        vectorizer_file_name = 'VECTORIZER_v1_' + file_name + '.pkl'\n",
    "        vectors_file_name = 'EMBEDDINGS_v1_' + file_name + '.pkl'\n",
    "\n",
    "        with open('../models/' + vectorizer_file_name, 'wb') as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "\n",
    "        with open('../models/' + vectors_file_name, 'wb') as file:\n",
    "            pickle.dump(vectors, file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('An error ocurred while trying to save the model. Error: ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f74f0b",
   "metadata": {},
   "source": [
    "# Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_features(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"len_tokens\"] = df[\"tokens\"].apply(lambda x: len(x))\n",
    "df = df[df[\"len_tokens\"] > 1].reset_index(drop=True)\n",
    "df = df.drop(columns=[\"len_tokens\"])\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # não estão pegando os tokens:\n",
    "# df = df[~df[\"host\"].isin([\"uoon.com.br\", \"psicomanager.com.br\"])].reset_index(drop=True)\n",
    "# df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599fc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1, stratify=df[\"segment\"])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexes = test_df.index.tolist()\n",
    "\n",
    "df_original = df.copy()\n",
    "df = df[~df.index.isin(test_indexes)].reset_index(drop=True)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "tfidf_matrix = cv_tfidf_vectorizer.fit_transform(token_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2184529",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_values = cv_tfidf_vectorizer.idf_\n",
    "feature_names = cv_tfidf_vectorizer.get_feature_names_out()\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'idf': idf_values})\n",
    "feature_importances = feature_importances.sort_values(by='idf', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=cv_tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "df = pd.concat([tfidf_df, df[\"segment\"]], axis=1)\n",
    "df_y = df[\"segment\"].values.tolist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector_by_segment = df.groupby(\"segment\").agg(\"mean\")\n",
    "mean_vector_by_segment = mean_vector_by_segment.iloc[mean_vector_by_segment.index.argsort()]\n",
    "mean_vector_by_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0267c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vector_by_segment.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_original[df_original.index.isin(test_indexes)].reset_index(drop=True)\n",
    "\n",
    "token_strings_test = [' '.join(doc) for doc in test_df['tokens']]\n",
    "tfidf_matrix_test = cv_tfidf_vectorizer.transform(token_strings_test)\n",
    "\n",
    "tfidf_df_test = pd.DataFrame(tfidf_matrix_test.toarray(), columns=cv_tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eacd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances(mean_vector, candidate_vectors):\n",
    "    mean_vector_values = np.asarray(mean_vector_by_segment.values.tolist())\n",
    "    candidate_vectors = candidate_vectors.values.tolist()\n",
    "    segments = mean_vector.index.to_list()\n",
    "    labels = []\n",
    "    probabilities = []\n",
    "\n",
    "    for vector in candidate_vectors:\n",
    "        distances = np.linalg.norm(mean_vector_values - np.asarray(vector), axis=1)\n",
    "        min_distance = np.argmin(distances)\n",
    "        labels.append(segments[min_distance])\n",
    "        probabilities.append(tuple((1/distances) / sum(1/distances)))\n",
    "    \n",
    "    # labels = np.asarray(labels).reshape(-1, 1).tolist()\n",
    "    return probabilities, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities, labels_predictions = calculate_distances(\n",
    "    mean_vector_by_segment,\n",
    "    tfidf_df_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = mean_vector_by_segment.index.to_list()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e049b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = test_df[\"segment\"].values.tolist()\n",
    "\n",
    "# best_estimator_score = best_estimator.score(X=best_indices_test_X, y=best_indices_test_Y)\n",
    "best_estimator_score = accuracy_score(y_true=df_y, y_pred=labels_predictions)\n",
    "best_estimator_score_f1 = f1_score(y_true=df_y, y_pred=labels_predictions, average=\"weighted\")\n",
    "\n",
    "best_estimator_cmatrix = confusion_matrix(y_pred=labels_predictions, y_true=df_y)\n",
    "best_estimator_creport = classification_report(y_pred=labels_predictions, y_true=df_y, zero_division=0, output_dict=True)\n",
    "\n",
    "print(f\"F1-Score: {round(best_estimator_score_f1, 4)}\")\n",
    "report(best_estimator_score, best_estimator_cmatrix, best_estimator_creport, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_features(cv_tfidf_vectorizer, mean_vector_by_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90916dd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(\"../data/iugu.parquet\", engine=\"pyarrow\")\n",
    "test_df[\"html\"] = test_df[\"html\"].astype(str)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52742a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_original = test_df[\"Segmento iugu\"]\n",
    "df_y_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d293825",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"host\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"url\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[[\"host\", \"url\", \"html\"]]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = generate_features(test_df)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# não estão pegando os tokens:\n",
    "test_df = test_df[~test_df[\"host\"].isin([\"uoon.com.br\", \"psicomanager.com.br\"])]\n",
    "test_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df_y_original[df_y_original.index.isin(test_df.index.tolist())]\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # não estão pegando os tokens (e às vezes o HTML também)\n",
    "# exclude_hosts = [\n",
    "#     \"vibx.com.br\",\n",
    "#     \"contabilizei.com.br\",\n",
    "#     \"vittude.com.br\",\n",
    "#     \"viatechinfo.com.br\",\n",
    "#     \"grupotravessia.com\",\n",
    "#     \"cursobeta.com.br\",\n",
    "#     \"supergeeks.com.br\",\n",
    "#     \"cursoyes.com.br\",\n",
    "#     \"braip.com\",\n",
    "#     \"kalyst.com.br\",\n",
    "#     \"plataforma.edibrasil.org\",\n",
    "# ]\n",
    "# test_df = test_df[~test_df[\"host\"].isin(exclude_hosts)].reset_index(drop=True)\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_path = \"../models/VECTORIZER_v1_iugu_tfidf_similarity_1000_eb_vinculados.pkl\"\n",
    "embeddings_path = \"../models/EMBEDDINGS_v1_iugu_tfidf_similarity_1000_eb_vinculados.pkl\"\n",
    "\n",
    "with open(vectorizer_path, \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open(embeddings_path, \"rb\") as f:\n",
    "    embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e18315",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = [' '.join(doc) for doc in test_df['tokens']]\n",
    "tfidf_matrix = vectorizer.transform(token_strings)\n",
    "\n",
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea74935",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities, labels_predictions = calculate_distances(\n",
    "    embeddings,\n",
    "    tfidf_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3feafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_0, y_probs_1, y_probs_2 = zip(*probabilities)\n",
    "y_probs_0 = list(y_probs_0)\n",
    "y_probs_1 = list(y_probs_1)\n",
    "y_probs_2 = list(y_probs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prob_Educação\"] = y_probs_0\n",
    "test_df[\"prob_SaaS\"] = y_probs_1\n",
    "test_df[\"prob_Saúde\"] = y_probs_2\n",
    "test_df[\"prediction\"] = labels_predictions\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(f1_score(\n",
    "    y_pred=test_df[\"prediction\"].values.tolist(),\n",
    "    y_true=df_y,\n",
    "    average=\"weighted\",\n",
    ")); print()\n",
    "\n",
    "pprint(classification_report(\n",
    "    y_pred=test_df[\"prediction\"].values.tolist(),\n",
    "    y_true=df_y,\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
