{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfd084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/iugu.parquet\", engine=\"pyarrow\")\n",
    "df[\"html\"] = df[\"html\"].astype(str)\n",
    "df[\"raiz_cnpj\"] = df[\"raiz_cnpj\"].astype(int)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_normalizado = pd.read_csv(\"/media/greca/HD/Driva/linkedin_normalizado_202505021514.csv\")\n",
    "linkedin_normalizado.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acfb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(linkedin_normalizado, on=\"raiz_cnpj\", how=\"left\")\n",
    "df = df[[\"url_x\", \"host\", \"html\", \"raiz_cnpj\", \"cnpj\", \"Nome da empresa\", \"Nicho Tech\", \"Segmento iugu\", \"url_y\", \"sobre\", \"slogan\", \"area_atuacao\"]]\n",
    "df = df.rename(columns={\"url_x\": \"url\", \"url_y\": \"linkedin_url\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d16457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['host'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844538c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Segmento iugu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Nicho Tech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f682e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e466503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"host\", \"url\", \"html\", \"sobre\", \"slogan\", \"area_atuacao\", \"Segmento iugu\"]]\n",
    "df = df.rename(columns={\"Segmento iugu\": \"segment\", \"sobre\": \"about\", \"area_atuacao\": \"field\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"about_is_null\"] = df[\"about\"].apply(lambda x: int(pd.isna(x)))\n",
    "df[\"slogan_is_null\"] = df[\"slogan\"].apply(lambda x: int(pd.isna(x)))\n",
    "df[\"field_is_null\"] = df[\"field\"].apply(lambda x: int(pd.isna(x)))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c14a5",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_integrity(dataframe):\n",
    "    try:\n",
    "        columns_expected = [\n",
    "            'host',\n",
    "            'html',\n",
    "            'url',\n",
    "        ]\n",
    "        \n",
    "        if not all(item in dataframe.columns.tolist() for item in columns_expected):\n",
    "            raise Exception('Missing required columns. Columns expected:\\n' + str(columns_expected))\n",
    "        \n",
    "        dataframe['html'] = dataframe['html'].astype(str)\n",
    "\n",
    "        dataframe_filtered = dataframe[(dataframe['html'] != '[]') & \n",
    "                                (dataframe['html'] != '')]\n",
    "    \n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with empty HTML and/or does not ends with '.br'. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "\n",
    "        dataframe_filtered = dataframe.drop_duplicates(subset=[\"host\"])\n",
    "        if len(dataframe) != len(dataframe_filtered):\n",
    "            count = len(dataframe) - len(dataframe_filtered)\n",
    "            print(f\"WARNING: dataframe has {count} entries with duplicates values. Removing those entries.\")\n",
    "            dataframe = dataframe_filtered\n",
    "    \n",
    "        nulls = dataframe['host'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'host' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['host'])\n",
    "\n",
    "        nulls = dataframe['url'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'url' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['url'])\n",
    "\n",
    "        nulls = dataframe['html'].isnull().sum()\n",
    "        if nulls > 0:\n",
    "            print(f\"WARNING: column 'html' has {nulls} empty values. Removing those entries.\")\n",
    "            dataframe = dataframe.dropna(subset=['html'])\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('Failed in integrity check.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemmatizer_pt_dict():\n",
    "    try:\n",
    "        import os\n",
    "        import requests\n",
    "        \n",
    "        url = \"https://github.com/michmech/lemmatization-lists/raw/master/lemmatization-pt.txt\"\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "\n",
    "        # Verificar se o arquivo já existe\n",
    "        if not os.path.exists(file_name):\n",
    "            response = requests.get(url)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "        # Processar o arquivo\n",
    "        lemmatizer_pt_dict = {}\n",
    "        with open(file_name, 'r') as dic:\n",
    "            for line in dic:\n",
    "                txt = line.split()\n",
    "                if len(txt) == 2:\n",
    "                    lemmatizer_pt_dict[txt[1]] = txt[0]\n",
    "\n",
    "        return lemmatizer_pt_dict\n",
    "    except Exception as e:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))\n",
    "\n",
    "    finally:\n",
    "        file_name = \"lemmatization-pt.txt\"\n",
    "        if os.path.exists(file_name):\n",
    "            os.remove(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lemmatizer(tokens, lemmatizer_pt_dict):\n",
    "    try:\n",
    "      from nltk.stem.wordnet import WordNetLemmatizer\n",
    "  \n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "      tokens_lemmatized = []\n",
    "      for token in tokens:\n",
    "        if token in lemmatizer_pt_dict.keys():\n",
    "          tokens_lemmatized.append(lemmatizer_pt_dict.get(token))\n",
    "        else:\n",
    "          tokens_lemmatized.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "      return tokens_lemmatized\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred on custom_lemmatizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_html_for_vectorizer(text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))\n",
    "\n",
    "def process_extra_text(text, lemmatizer_pt_dict):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import unicodedata\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    try:              \n",
    "        STOP_WORDS = (set(stopwords.words('portuguese'))).union(set(stopwords.words('english')))\n",
    "\n",
    "        preprocessed_text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "        # Remover espaços em branco e quebras de linha desnecessárias\n",
    "        preprocessed_text = re.sub(r'\\s+', ' ', preprocessed_text).strip()\n",
    "\n",
    "        # substitui tudo que não é letra ou espaço por um espaço\n",
    "        preprocessed_text = re.sub(r\"[^a-zA-Z\\s]\", \" \", preprocessed_text)\n",
    "\n",
    "        # Regex para identificar palavras\n",
    "        pattern = re.compile(r'([A-Z]+(?![a-z])|[A-Z][a-z]*|[a-z]+)')\n",
    "\n",
    "        # Substituir as correspondências por elas mesmas precedidas por um espaço\n",
    "        preprocessed_text = pattern.sub(r' \\1', preprocessed_text)\n",
    "\n",
    "        # lowercase\n",
    "        preprocessed_text = preprocessed_text.lower()\n",
    "\n",
    "        # remover possives espaços repetidos\n",
    "        preprocessed_text = re.sub(r\"\\s+\", \" \", preprocessed_text).strip()\n",
    "\n",
    "        # tokenizar\n",
    "        tokens = nltk.word_tokenize(preprocessed_text)\n",
    "\n",
    "        # remover stopwords\n",
    "        tokens = [\n",
    "            token for token in tokens if token not in STOP_WORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Aplicar lemmatizer\n",
    "        tokens = custom_lemmatizer(tokens, lemmatizer_pt_dict)\n",
    "\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for vectorizer.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def process_html_for_how_many_prices(text):\n",
    "    try:              \n",
    "        regex_precos = re.compile(r'\\$|R\\$')\n",
    "        precos = regex_precos.findall(text)\n",
    "        return len(precos)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for prices.\\nError:\\n' + str(e))\n",
    "\n",
    "def process_html_for_how_many_values(text):\n",
    "    try:              \n",
    "        regex_valores = re.compile(r'\\d+(?:\\.\\d{3})*(?:,\\d{2})?|\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
    "        valores = regex_valores.findall(text)\n",
    "        return len(valores)\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occurred while processing HTMLs for values.\\nError:\\n' + str(e))\n",
    "\n",
    "def get_html_body(html_str):\n",
    "    try:\n",
    "        # Tentar usar diferentes parsers\n",
    "        for parser in ['html.parser', 'html5lib', 'lxml']:\n",
    "            try:\n",
    "                soup = BeautifulSoup(html_str, parser)\n",
    "                text = soup.body.get_text() if soup.body else ''\n",
    "                return text\n",
    "            except Exception as parser_e:\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_number(text):\n",
    "    text = re.sub(r'[^\\d]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_invalid_company(company_id):\n",
    "    company_id = re.sub(r'(\\d)\\1{12}', '', company_id)\n",
    "    if len(company_id) == 14:\n",
    "        return company_id\n",
    "    return None \n",
    "\n",
    "def order_by_common(data):\n",
    "    from collections import Counter\n",
    "    data_output = Counter(data)\n",
    "    return [k for k, v in data_output.most_common()]\n",
    "\n",
    "def extract_and_process_cnpjs(text):\n",
    "    pattern = re.compile(r'\\d{2}\\.\\d{3}\\.\\d{3}[\\/ ]\\d{4}[- ]\\d{2}')\n",
    "    matches = pattern.findall(text)\n",
    "    processed_matches = []\n",
    "    for match in matches:\n",
    "        cleaned = only_number(match)\n",
    "        valid_company = remove_invalid_company(cleaned)\n",
    "        if valid_company:\n",
    "            processed_matches.append(valid_company)\n",
    "    return processed_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(dataframe):\n",
    "    try:\n",
    "        dataframe = check_integrity(dataframe)\n",
    "\n",
    "        lem_dict = build_lemmatizer_pt_dict()\n",
    "\n",
    "        dataframe.loc[:, 'html_about'] = dataframe.loc[:,'about'].apply(lambda x: process_extra_text(x, lem_dict))\n",
    "        dataframe.loc[:, 'html_slogan'] = dataframe.loc[:,'slogan'].apply(lambda x: process_extra_text(x, lem_dict))\n",
    "        \n",
    "        html_body = dataframe.loc[:,'html'].apply(get_html_body)\n",
    "        dataframe.loc[:, 'html_tokens'] = html_body.apply(lambda x: process_html_for_vectorizer(x, lem_dict))\n",
    "\n",
    "        dataframe = dataframe.drop(columns=['about', 'slogan', 'html'])\n",
    "        dataframe['tokens'] = dataframe[['html_about', 'html_slogan', 'html_tokens']].sum(axis=1)\n",
    "        \n",
    "        return dataframe\n",
    "    except Exception as e:\n",
    "        raise Exception('An error occured while trying to generate features.\\nError:\\n' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(score, confusion_matrix, classification_report, model_card, classes):\n",
    "    \n",
    "    # Gera o heatmap da confusion matrix\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt=\"d\", \n",
    "                linewidths=.5, \n",
    "                square = True, \n",
    "                cmap = 'Blues', \n",
    "                annot_kws={\"size\": 16}, \n",
    "                xticklabels=classes, \n",
    "                yticklabels=classes)\n",
    "\n",
    "    plt.xticks(rotation='horizontal', fontsize=16)\n",
    "    plt.yticks(rotation='horizontal', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', size=20)\n",
    "    plt.ylabel('Actual Label', size=20)\n",
    "\n",
    "    title = 'Accuracy Score: {0:.4f}'.format(score)\n",
    "    plt.title(title, size = 20)\n",
    "\n",
    "    # Mostra o classification report e o heatmap\n",
    "    pprint(classification_report)\n",
    "    plt.show()\n",
    "\n",
    "    model_card['accuracy_best'] = round(classification_report['accuracy'], 4)\n",
    "    model_card['precision_macro_best'] = round(classification_report['macro avg']['precision'], 4)\n",
    "    model_card['recall_macro_best'] = round(classification_report['macro avg']['recall'], 4)\n",
    "    model_card['f1_macro_best'] = round(classification_report['macro avg']['f1-score'], 4)\n",
    "    model_card['support_0_best'] = classification_report['0']['support']\n",
    "    model_card['support_1_best'] = classification_report['1']['support']\n",
    "    model_card['support_2_best'] = classification_report['2']['support']\n",
    "\n",
    "    return model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf733bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_report(cross_validate_results, model_card):\n",
    "\n",
    "    # métricas dos modelos gerados no cross validation\n",
    "    print('accuracy:\\t', cross_validate_results['test_accuracy'], ' \\tmean: ', cross_validate_results['test_accuracy'].mean())\n",
    "    print('precision:\\t', cross_validate_results['test_precision'], ' \\tmean: ', cross_validate_results['test_precision'].mean())\n",
    "    print('recall:\\t\\t', cross_validate_results['test_recall'], ' \\tmean: ', cross_validate_results['test_recall'].mean())\n",
    "    print('f1:\\t\\t', cross_validate_results['test_f1'], ' \\tmean: ', cross_validate_results['test_f1'].mean())\n",
    "    print('fit_time:\\t', cross_validate_results['fit_time'], ' \\tmean: ', cross_validate_results['fit_time'].mean())\n",
    "    print('score_time:\\t', cross_validate_results['score_time'], ' \\tmean: ', cross_validate_results['score_time'].mean())\n",
    "\n",
    "    max_f1_pos = list(cross_validate_results['test_f1']).index(max(cross_validate_results['test_f1']))\n",
    "    best_estimator = cross_validate_results['estimator'][max_f1_pos]\n",
    "    best_indices = {\n",
    "        'train': cross_validate_results['indices']['train'][max_f1_pos],\n",
    "        'test': cross_validate_results['indices']['test'][max_f1_pos]\n",
    "    }\n",
    "\n",
    "    model_card['accuracy_mean'] = round(cross_validate_results['test_accuracy'].mean(), 4)\n",
    "    model_card['precision_mean'] = round(cross_validate_results['test_precision'].mean(), 4)\n",
    "    model_card['recall_mean'] = round(cross_validate_results['test_recall'].mean(), 4)\n",
    "    model_card['f1_mean'] = round(cross_validate_results['test_f1'].mean(), 4)\n",
    "    model_card['fit_time_mean'] = round(cross_validate_results['fit_time'].mean(), 4)\n",
    "    model_card['score_time_mean'] = round(cross_validate_results['score_time'].mean(), 4)\n",
    "\n",
    "    \n",
    "    return best_estimator, best_indices, model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectorizer_model(vectorizer, model, encoder, model_card):\n",
    "    try:\n",
    "        file_name = model_card['scope'] + '_' + model_card['vectorizer'] + '_' + model_card['model'] + '_' + \\\n",
    "            str(model_card['word_reduction']) + '_' + str(model_card['kfold_splits']) + '_' + str(model_card['kfold_shuffle']) + '_' + \\\n",
    "                str(model_card['kfold_random_state']) + '_' + str(model_card['vectorizer_max_features']) + '_' + model_card['dataset']\n",
    "\n",
    "        model_card['vectorizer_file_name'] = 'VECTORIZER_v1_' + file_name + '.pkl'\n",
    "        model_card['encoder_file_name'] = 'ENCODER_v1_' + file_name + '.pkl'\n",
    "        model_card['model_file_name'] = 'MODEL_v1_' + file_name + '.pkl'\n",
    "\n",
    "        with open('../models/' + model_card['vectorizer_file_name'], 'wb') as file:\n",
    "            pickle.dump(vectorizer, file)\n",
    "\n",
    "        with open('../models/' + model_card['model_file_name'], 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "        \n",
    "        with open('../models/' + model_card['encoder_file_name'], 'wb') as file:\n",
    "            pickle.dump(encoder, file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('An error ocurred while trying to save the model. Error: ' + str(e))\n",
    "        print(model_card)\n",
    "\n",
    "    return model_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = {\n",
    "    'scope': 'iugu',\n",
    "    'vectorizer': '',\n",
    "    'model': '',\n",
    "    'word_reduction': 'custom_lemmatizer', \n",
    "    'kfold_splits': 3,\n",
    "    'kfold_shuffle': True,\n",
    "    'kfold_random_state': 42,\n",
    "    'vectorizer_max_features': 1000, # None ou INTeger -> testado: None, 50, 100, 200, 500, 1000, 1500, 2000\n",
    "    'dataset': 'base_iugu', \n",
    "    'accuracy_mean': '',\n",
    "    'precision_mean': '',\n",
    "    'recall_mean': '',\n",
    "    'f1_mean': '',\n",
    "    'fit_time_mean': '',\n",
    "    'score_time_mean': '',\n",
    "    'accuracy_best': '',\n",
    "    'precision_macro_best': '',\n",
    "    'recall_macro_best': '',\n",
    "    'f1_macro_best': '',\n",
    "    'support_0_best': '',\n",
    "    'support_1_best': '',\n",
    "    'support_2_best': '',\n",
    "    'vectorizer_file_name': '',\n",
    "    'model_file_name': '',\n",
    "    'encoder_file_name': '',\n",
    "}\n",
    "\n",
    "# metricas utilizadas pela validação cruzada\n",
    "scoring_metrics = {\n",
    "    'accuracy': make_scorer(accuracy_score, normalize=True), \n",
    "    'precision': make_scorer(precision_score, average='macro', zero_division=0), \n",
    "    'recall': make_scorer(recall_score, average='macro'), \n",
    "    'f1': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "# CV splitter com StratifiedKFold, para manter proporções de exemplos de cada classe target\n",
    "stratified_kfold = StratifiedKFold(n_splits=model_card['kfold_splits'], shuffle=model_card['kfold_shuffle'], random_state=model_card['kfold_random_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f74f0b",
   "metadata": {},
   "source": [
    "# Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_features(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluir os que não estão pegando os tokens\n",
    "df['len_tokens'] = df['tokens'].apply(len)\n",
    "df = df[df['len_tokens'] > 0].reset_index(drop=True)\n",
    "df = df.drop(columns=['len_tokens'])\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df['field']).astype(int)], axis=1)\n",
    "df = df.drop(columns=['field'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5484c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599fc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive-Bayes com TF-IDF\n",
    "mnb_tfidf_model_card = model_card.copy()\n",
    "mnb_tfidf_model_card['vectorizer'] = 'tfidf_vectorizer'\n",
    "mnb_tfidf_model_card['model'] = 'multinomial_nb'\n",
    "\n",
    "cv_tfidf_vectorizer = TfidfVectorizer(max_features=model_card['vectorizer_max_features'])\n",
    "token_strings = [' '.join(doc) for doc in df['tokens']]\n",
    "tfidf_matrix = cv_tfidf_vectorizer.fit_transform(token_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2184529",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_values = cv_tfidf_vectorizer.idf_\n",
    "feature_names = cv_tfidf_vectorizer.get_feature_names_out()\n",
    "feature_importances = pd.DataFrame({'feature': feature_names, 'idf': idf_values})\n",
    "feature_importances = feature_importances.sort_values(by='idf', ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=cv_tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "features = ['about_is_null', 'slogan_is_null', 'field_is_null']\n",
    "\n",
    "# Concatenar os dataframes de features\n",
    "other_features = df[features]\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "\n",
    "df_y_original = df[\"segment\"].copy()\n",
    "\n",
    "df[\"segment\"] = le.fit_transform(df[\"segment\"])\n",
    "df_y = df[\"segment\"]\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03c780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01893f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afaf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0751d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "# clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# cross validation\n",
    "cv_results = cross_validate(estimator=clf, X=features_df, y=df_y, \n",
    "                            cv=stratified_kfold, scoring=scoring_metrics,\n",
    "                            return_estimator=True, return_indices=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validate report\n",
    "best_estimator, best_indices, mnb_tfidf_model_card = cross_validate_report(cv_results, mnb_tfidf_model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e049b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model report\n",
    "best_indices_test_X = features_df.iloc[best_indices['test']]\n",
    "best_indices_test_Y = df_y.iloc[best_indices['test']]\n",
    "\n",
    "best_estimator_predictions = list(best_estimator.predict(best_indices_test_X))\n",
    "best_indices_test_Y = best_indices_test_Y.values.tolist()\n",
    "\n",
    "# best_estimator_score = best_estimator.score(X=best_indices_test_X, y=best_indices_test_Y)\n",
    "best_estimator_score = accuracy_score(y_true=best_indices_test_Y, y_pred=best_estimator_predictions)\n",
    "best_estimator_score_f1 = f1_score(y_true=best_indices_test_Y, y_pred=best_estimator_predictions, pos_label=0, average=\"weighted\")\n",
    "\n",
    "best_estimator_cmatrix = confusion_matrix(y_pred=best_estimator_predictions, y_true=best_indices_test_Y)\n",
    "best_estimator_creport = classification_report(y_pred=best_estimator_predictions, y_true=best_indices_test_Y, zero_division=0, output_dict=True)\n",
    "\n",
    "mnb_tfidf_model_card = model_report(best_estimator_score, best_estimator_cmatrix, best_estimator_creport, mnb_tfidf_model_card, list(le.classes_))\n",
    "\n",
    "# save model\n",
    "mnb_tfidf_model_card = save_vectorizer_model(cv_tfidf_vectorizer, clf, le, mnb_tfidf_model_card)\n",
    "\n",
    "print(f\"F1-Score: {round(best_estimator_score_f1, 4)}\")\n",
    "print(mnb_tfidf_model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c315b3",
   "metadata": {},
   "source": [
    "# Train full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "# clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "clf.fit(features_df, df_y)\n",
    "mnb_tfidf_model_card = save_vectorizer_model(cv_tfidf_vectorizer, clf, le, mnb_tfidf_model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90916dd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(\"../data/iugu_enrichment.parquet\", engine=\"pyarrow\")\n",
    "test_df[\"html\"] = test_df[\"html\"].astype(str)\n",
    "test_df[\"raiz_cnpj\"] = test_df[\"raiz_cnpj\"].astype(int)\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(linkedin_normalizado, on=\"raiz_cnpj\", how=\"left\")\n",
    "test_df = test_df[[\"url_x\", \"host\", \"html\", \"raiz_cnpj\", \"cnpj\", \"Nome da empresa\", \"url_y\", \"sobre\", \"slogan\", \"area_atuacao\"]]\n",
    "test_df = test_df.rename(columns={\"url_x\": \"url\", \"url_y\": \"linkedin_url\"})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b4341",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d293825",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"host\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"url\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a6d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[[\"host\", \"url\", \"html\", \"sobre\", \"slogan\", \"area_atuacao\"]]\n",
    "test_df = test_df.rename(columns={\"Segmento iugu\": \"segment\", \"sobre\": \"about\", \"area_atuacao\": \"field\"})\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"about_is_null\"] = test_df[\"about\"].apply(lambda x: int(pd.isna(x)))\n",
    "test_df[\"slogan_is_null\"] = test_df[\"slogan\"].apply(lambda x: int(pd.isna(x)))\n",
    "test_df[\"field_is_null\"] = test_df[\"field\"].apply(lambda x: int(pd.isna(x)))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a858cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = generate_features(test_df)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "test_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluir os que não estão pegando os tokens\n",
    "test_df['len_tokens'] = test_df['tokens'].apply(len)\n",
    "test_df = test_df[test_df['len_tokens'] > 0].reset_index(drop=True)\n",
    "test_df = test_df.drop(columns=['len_tokens'])\n",
    "test_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_df, pd.get_dummies(test_df['field']).astype(int)], axis=1)\n",
    "test_df = test_df.drop(columns=['field'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/MODEL_v1_iugu_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_base_iugu.pkl\"\n",
    "vectorizer_path = \"../models/VECTORIZER_v1_iugu_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_base_iugu.pkl\"\n",
    "encoder_path = \"../models/ENCODER_v1_iugu_tfidf_vectorizer_multinomial_nb_custom_lemmatizer_3_True_42_1000_base_iugu.pkl\"\n",
    "\n",
    "with open(vectorizer_path, \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(encoder_path, \"rb\") as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e18315",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = [' '.join(doc) for doc in test_df['tokens']]\n",
    "tfidf_matrix = vectorizer.transform(token_strings)\n",
    "\n",
    "# Converter a matriz TF-IDF em um dataframe pandas\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "features = ['about_is_null', 'slogan_is_null', 'field_is_null']\n",
    "\n",
    "# Concatenar os dataframes de features\n",
    "other_features = test_df[features]\n",
    "features_df = pd.concat([other_features, tfidf_df], axis=1)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"prediction\"] = list(best_estimator.predict(features_df))\n",
    "test_df[\"prediction\"] = test_df[\"prediction\"].apply(lambda x: encoder.inverse_transform([x])[0])\n",
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driva_ecomm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
